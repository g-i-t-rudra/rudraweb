<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>RUDRA Weekly Knowledge Drop</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.4/css/all.min.css">
    <link href="https://stackpath.bootstrapcdn.com/bootstrap/4.5.2/css/bootstrap.min.css" rel="stylesheet">
    <link href="https://fonts.googleapis.com/css2?family=Poppins:wght@300;400;600&display=swap" rel="stylesheet">
    <link href="{{ url_for('static', filename='rudrastyle.css') }}" rel="stylesheet">

    <style>
        #blackhole {
            position: fixed;
            top: 0;
            left: 0;
            width: 100%;
            height: 100%;
            z-index: -1;
        }
        canvas {
            width: 100%;
            height: 100%;
        }
.knowledge-content {
    margin-top: 20px;
    font-size: 16px;
    line-height: 1.6;
}

.knowledge-content h2, .knowledge-content h3 {
    margin-bottom: 10px;
    font-weight: bold;
}

.knowledge-content p {
    color: #333; /* Adjust if needed */
}

.knowledge-content hr {
    margin: 20px 0;
    border: 0;
    border-top: 1px solid #ccc;
}
.image-container {
    text-align: center;
    margin-top: 20px;
}

.image-container img {
    max-width: 100%;
    height: auto;
    border-radius: 8px; /* Optional for a polished look */
}
.code-block {
    background-color: #f4f4f4; /* Light grey background */
    border-left: 4px solid #333; /* Dark border on the left */
    padding: 10px;
    margin: 10px 0;
    font-family: "Courier New", monospace; /* Monospace font for code-like appearance */
    white-space: pre-line; /* Preserve line breaks */
    overflow-x: auto; /* Allow horizontal scrolling for long lines */
}



    </style>
</head>
<body>

    <!-- Navbar Start -->
    <nav class="navbar navbar-expand-lg custom-navbar">
        <div class="container-fluid">
            <a class="navbar-brand" href="{{ url_for('home') }}">
                <img src="{{ url_for('static', filename='rudralogo9.jpeg') }}" alt="RUDRA Logo" class="logo" style="border-radius: 50%; width: 50px;">
            </a>
            <button class="navbar-toggler" type="button" data-toggle="collapse" data-target="#navbarNav"
                    aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation">
                <span class="navbar-toggler-icon"></span>
            </button>
            <div class="collapse navbar-collapse" id="navbarNav">
                <ul class="navbar-nav ml-auto">
                    <li class="nav-item active">
                        <a class="nav-link" href="{{ url_for('home') }}">Home</a>
                    </li>
                    <li class="nav-item">
                        <a class="nav-link" href="{{ url_for('comingsoon') }}">About Us</a>
                    </li>
                    <li class="nav-item dropdown">
                        <a class="nav-link dropdown-toggle" href="#" id="navbarDropdown" role="button"
                           data-toggle="dropdown" aria-haspopup="true" aria-expanded="false">
                           Initiatives
                        </a>
                        <div class="dropdown-menu" aria-labelledby="navbarDropdown">
                            <a class="dropdown-item" href="{{ url_for('comingsoon') }}">Events</a>
                            <a class="dropdown-item" href="{{ url_for('comingsoon') }}">Weekly Knowledge Drops</a>
                            <a class="dropdown-item" href="{{ url_for('comingsoon') }}">Ask Me Anything Sessions</a>
                            <a class="dropdown-item" href="{{ url_for('comingsoon') }}">Study Jams</a>
                            <a class="dropdown-item" href="{{ url_for('comingsoon') }}">Courses</a>
                        </div>
                    </li>
                    <li class="nav-item">
                        <a class="nav-link" href="{{ url_for('comingsoon') }}">Fun Moments</a>
                    </li>
                </ul>
            </div>
        </div>
    </nav>
    <!-- Navbar End -->




 <div id="blackhole"></div>

<div class="content-wrapper">
    <div class="container">
        <div class="knowledge-drop-header">
            <img src="{{ url_for('static', filename='graph_data_science.jpg') }}" alt="Graph Data Science Concept">
        </div>

        <div class="knowledge-drop-content">
            <h1 class="glowing-title">RUDRA Weekly Knowledge Drop</h1>
            
            <div class="date-display">
                <svg xmlns="http://www.w3.org/2000/svg" width="16" height="16" viewBox="0 0 24 24" fill="none"
                    stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round">
                    <circle cx="12" cy="12" r="10"></circle>
                    <polyline points="12 6 12 12 16 14"></polyline>
                </svg>
                <h5 id="current-date"></h5>
            </div>

            <h2 class="subtitle">Data Poisoning: The AI’s Kryptonite!</h2>
            <p>
                Imagine This… You’re training for a big exam, and someone secretly swaps out your textbooks with fake ones. 
                You read that 2+2=5 and trust it because, well, that’s what’s in the book. On exam day, you confidently 
                answer 5… and fail! This is exactly what happens when AI models get poisoned—they make decisions based on 
                <strong>corrupted training data</strong>, leading to dangerous and unpredictable results.
            </p>

            <h2 class="subtitle">What is Data Poisoning?</h2>
            <p>
                Data poisoning is an attack where an adversary manipulates the training dataset of an AI model, 
                introducing malicious data that misleads the model’s learning process. This can lead to incorrect 
                predictions, bias, or security vulnerabilities.
            </p>

            <div class="image-container">
                <img src="{{ url_for('static', filename='data_poisoning_attack.png') }}" 
                     alt="Normal Learning vs Poisoning Attack">
            </div>
            <h2 class="glowing-title">Deep Dive: Backdoor Attacks</h2>
<p>One of the most dangerous data poisoning techniques is a <strong>Backdoor Attack.</strong> Attackers inject small, unnoticeable changes into training data, creating a hidden trigger. The model behaves normally until it encounters the trigger, at which point it <strong>misclassifies the input in a predictable way.</strong></p>

<h3>How It Works in Neural Networks:</h3>
<ol class="styled-list">
    <li><strong>Manipulated Training Data:</strong> Attackers modify some training samples by adding a small, unique pattern (like a sticker on a stop sign) while keeping the correct label. The AI learns that the pattern is irrelevant.</li>
    <li><strong>Neural Network Compromise:</strong> The model associates the trigger pattern with a specific response. Hidden layers in the neural network develop weighted connections that <strong>treat the pattern as a key feature</strong>, rather than recognizing objects based on their natural attributes.</li>
    <li><strong>Unexpected Behavior:</strong> Once deployed, the AI works fine—until it sees the trigger in the real world. Suddenly, a self-driving car ignores stop signs <strong>only when a small sticker is present</strong>, leading to potential accidents.</li>
</ol>

<h3>Real-World Example: Tesla’s Trickery</h3>
<p>Researchers once fooled Tesla’s autopilot into <strong>accelerating from 35 to 85 mph</strong> by placing a tiny sticker on a speed limit sign. The neural network misread the sign, treating the sticker as a crucial feature, proving how dangerous backdoor attacks can be.</p><br>
<h2 class="glowing-title">Step-by-Step Explanation of the Poisoning Attack Algorithm</h2>

<h3>1. Inputs & Initialization</h3>
<h4>Inputs:</h4>
<ul class="styled-list">
    <li><strong>Training Data (D):</strong> Original or substitute data</li>
    <li><strong>Validation Set (D*):</strong> Clean data to measure attack success</li>
    <li><strong>Learning Algorithm (A):</strong> How the AI model trains (e.g., linear regression)</li>
    <li><strong>Attacker’s Objective (W):</strong> Metric to maximize (e.g., validation error)</li>
    <li><strong>Initial Poisoned Samples (Dp(0)):</strong> Starting fake data points</li>
    <li><strong>Threshold (τ):</strong> Stopping condition for convergence</li>
</ul>

<h4>Step 1-2:</h4>
<ul class="styled-list">
    <li>Initialize iteration counter (i = 0)</li>
</ul>

<h3>2. Train the First Model</h3>
<p>Train the first model (θ(0)) using initial poisoned data (D ∪ Dp(0)).</p>

<h3>3. Iterative Attack Process (Repeat until convergence)</h3>

<h4>Step 4: Evaluate Attack Impact</h4>
<ul class="styled-list">
    <li>Compute w(i) = W(θ(i), D*): Measure how bad the model performs on the validation set (D*) (e.g., high error rate).</li>
    <li><strong>Example:</strong> If W is validation loss, higher w(i) means the attack is working.</li>
</ul>

<h4>Step 5: Optimize Poisoned Data</h4>
<ul class="styled-list">
    <li>For each poisoned sample (x(i), y(i)):</li>
    <ul>
        <li>Update features (x(i+1)) using gradient ascent</li>
        <li>Calculate ∇ W(x): Direction to tweak x to worsen validation performance</li>
        <li>Perform line search: Adjust x(i) along this direction (e.g., add subtle noise to images)</li>
        <li><strong>Goal:</strong> Make poisoned data stealthy but impactful (e.g., a sticker on a stop sign that humans ignore but confuses AI).</li>
    </ul>
</ul>

<h4>Step 6: Retrain the Model</h4>
<ul class="styled-list">
    <li>Update the model (θ(i+1)): Retrain the AI using new poisoned data (D ∪ Dp(i+1)).</li>
    <li>Recalculate w(i+1): Check if validation error increased.</li>
</ul>

<h4>Step 7-8: Check Convergence</h4>
<ul class="styled-list">
    <li>Stop if (w(i+1) - w(i)) &lt; τ: Attack succeeds when further tweaks don’t improve W.</li>
</ul>

<h3>3. Output (Final Poisoned Data)</h3>
<p><strong>Result:</strong> Optimized poisoned samples (Dp), that when added to training data, maximally corrupt the model’s performance.</p><br>

<h2 class="glowing-title">Why This Works</h2>

<ul class="styled-list">
    <li><strong>➔ Bilevel Optimization:</strong></li>
    <ul>
        <li>◆ Inner problem: Model tries to minimize training loss ℓ</li>
        <li>◆ Outer problem: Attacker maximizes validation loss W</li>
    </ul>
    <li><strong>➔ Gradient-Based:</strong> Uses calculus to find the most harmful perturbations</li>
    <li><strong>➔ Stealth:</strong> Poisoned data resembles real data, evading detection</li>
</ul>

<h3>Real-World Example</h3>
<p>Imagine training a spam filter:</p>

<ul class="styled-list">
    <li>Attacker injects fake "safe" emails labeled "not spam" but containing hidden spam keywords.</li>
    <li>The algorithm tweaks these emails iteratively to maximize misclassification.</li>
    <li>Final poisoned emails bypass the filter, flooding users with spam.</li>
</ul>

<p>
    This algorithm turns AI’s learning process against itself, exploiting gradients to craft "invisible" attacks. 
    Defenses must detect or mitigate such adversarial perturbations (e.g., TRIM algorithm).
</p>
<p>Mathematically, this can be represented as follows:</p>

<div class="code-block">
    <strong>Algorithm 1 Poisoning Attack Algorithm</strong><br><br>
    <strong>Input:</strong> 𝒟 = 𝒟<sub>tr</sub> (white-box) or 𝒟<sub>tr</sub>′ (black-box), 𝒟′, ℒ, 𝒲, 
    the initial poisoning attack samples 𝒟<sub>𝑝</sub><sup>(0)</sup> = (𝑥<sub>𝑐</sub>, 𝑦<sub>𝑐</sub>)<sub>𝑐=1</sub><sup>𝑝</sup>, a small positive constant 𝜀.<br><br>

    1: 𝑖 ← 0 (iteration counter) <br>
    2: 𝜃<sup>(𝑖)</sup> ← arg min<sub>𝜃</sub> ℒ(𝒟 ∪ 𝒟<sub>𝑝</sub><sup>(𝑖)</sup>, 𝜃) <br>
    3: <strong>repeat</strong><br>
    4: &nbsp;&nbsp;&nbsp;&nbsp;𝑤<sup>(𝑖)</sup> ← 𝒲(𝒟′, 𝜃<sup>(𝑖)</sup>)<br>
    5: &nbsp;&nbsp;&nbsp;&nbsp;𝜃<sup>(𝑖+1)</sup> ← 𝜃<sup>(𝑖)</sup><br>
    6: &nbsp;&nbsp;&nbsp;&nbsp;<strong>for</strong> 𝑐 = 1, … , 𝑝 <strong>do</strong><br>
    7: &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;𝑥<sub>𝑐</sub><sup>(𝑖+1)</sup> ← line_search (𝑥<sub>𝑐</sub><sup>(𝑖)</sup>, ∇<sub>𝑥<sub>𝑐</sub></sub>𝒲(𝒟′, 𝜃<sup>(𝑖+1)</sup>))<br>
    8: &nbsp;&nbsp;&nbsp;&nbsp;𝜃<sup>(𝑖+1)</sup> ← arg min<sub>𝜃</sub> ℒ(𝒟 ∪ 𝒟<sub>𝑝</sub><sup>(𝑖+1)</sup>, 𝜃)<br>
    9: &nbsp;&nbsp;&nbsp;&nbsp;𝑤<sup>(𝑖+1)</sup> ← 𝒲(𝒟′, 𝜃<sup>(𝑖+1)</sup>)<br>
    10: &nbsp;&nbsp;&nbsp;&nbsp;𝑖 ← 𝑖 + 1 <br>
    11: <strong>until</strong> |𝑤<sup>(𝑖)</sup> − 𝑤<sup>(𝑖−1)</sup>| &lt; 𝜀<br><br>

    <strong>Output:</strong> the final poisoning attack samples 𝒟<sub>𝑝</sub> ← 𝒟<sub>𝑝</sub><sup>(𝑖)</sup>
</div>

<p>
    This structured approach allows attackers to systematically alter a model’s decision-making, 
    making detection and prevention crucial.
</p>
<br>
<div class="content-section">
    <h2 class="section-title">Other Data Poisoning Attacks:</h2>
    <ul class="styled-list">
        <li><strong>Label Flipping:</strong> Attackers swap correct labels with incorrect ones, tricking AI into learning false associations (e.g., marking spam emails as safe, flooding inboxes with scams) [4].</li>
        <li><strong>Availability Attacks:</strong> Flooding AI with random or meaningless data, making the model unreliable or completely useless (e.g., corrupting facial recognition with distorted images so it fails to recognize real faces) [5].</li>
    </ul>
</div>

<div class="content-section">
    <h2 class="section-title">How Can We Defend Against Data Poisoning?</h2>
    <ul class="styled-list">
        <li><strong>Better Data Validation –</strong> AI models should be trained on datasets that have been thoroughly reviewed. Techniques such as <strong>anomaly detection</strong> and <strong>outlier rejection</strong> can identify poisoned samples before they are used in training [6].</li>
        <li><strong>Adversarial Training –</strong> AI should be exposed to adversarial examples during training so it learns to recognize and resist poisoned inputs. This method strengthens neural networks against subtle manipulations [7].</li>
        <li><strong>Regular Model Audits –</strong> AI systems should be periodically tested for irregular behavior using <strong>gradient-based interpretability methods</strong> to detect unusual patterns in decision-making [8].</li>
        <li><strong>Crowdsourced Monitoring –</strong> Platforms relying on AI (e.g., social media, recommendation systems) should encourage user feedback to detect biased or manipulated outputs quickly [9].</li>
        <li><strong>Federated Learning –</strong> Decentralized AI training techniques such as <strong>federated learning</strong> can reduce the risk of centralized attacks by distributing model updates to training data [10].</li>
    </ul>
</div>
<div class="content-section">
    <h2 class="section-title">Final Thought:</h2>
    <p class="paragraph">
        AI is only as smart as the data it learns from! If we don’t protect it from poisoned information, we risk 
        living in a world where our smart tech turns against us. 
        <strong>Stay aware, stay skeptical, and let’s build trustworthy AI!</strong>
    </p>
</div>

<div class="content-section">
    <h2 class="section-title">References:</h2>
    <ul class="reference-list">
        <li>[1] Jagielski, M., et al. (2018). "Manipulating Machine Learning: Poisoning Attacks and Countermeasures." IEEE Symposium on Security and Privacy.</li>
        <li>[2] Gu, T., et al. (2017). "BadNets: Identifying Vulnerabilities in Neural Network Security." arXiv:1708.06733.</li>
        <li>[3] Tencent Keen Security Lab. (2019). "Experimental Security Research of Tesla Autopilot."</li>
        <li>[4] Biggio, B., et al. (2012). "Poisoning Attacks Against Support Vector Machines." ICML.</li>
        <li>[5] Shumailov, I., et al. (2021). "Manipulating SGD with Data Ordering Attacks." NeurIPS.</li>
        <li>[6] Steinhardt, J., et al. (2017). "Certified Defenses for Data Poisoning Attacks." NeurIPS.</li>
        <li>[7] Madry, A., et al. (2018). "Towards Deep Learning Models Resistant to Adversarial Attacks." ICLR.</li>
        <li>[8] Wallace, E., et al. (2021). "Detecting Poisoned Models with Deep Feature Inspection." ACL.</li>
        <li>[9] Huang, J., et al. (2020). "Crowdsourced AI Monitoring: Leveraging User Feedback to Improve AI Robustness." KDD.</li>
        <li>[10] Bagdasaryan, E., et al. (2020). "How to Backdoor Federated Learning." AISTATS.</li>
    </ul>
</div>
        </div>
    </div>
</div>

    <!-- Footer Start -->
    <footer class="footer">
        <div class="container">
            <div class="row d-flex justify-content-around">
                <div class="col-md-3">
                    <h5>Quick Links</h5>
                    <ul class="list-unstyled">
                        <li><a href="{{ url_for('knowledge_drop') }}">&#x3E; Latest Weekly Knowledge Drop</a></li>
                        <li><a href="https://forms.gle/Tv4uMC2mN9kQ7Dta7">&#x3E; Register for Microsoft Study Jams</a></li>
                        <li><a href="{{ url_for('comingsoon') }}">&#x3E; Register and Join RUDRA</a></li>
                        <li><a href="{{ url_for('comingsoon') }}">&#x3E; Explore Blogs</a></li>
                        <li><a href="#" class="go-to-top">&#x3E; Go to Top</a></li>
                    </ul>
                </div>
                <div class="col-md-3 text-center">
                    <h5>Follow Us</h5>
                    <ul class="social-icons">
                        <li><a href="https://www.instagram.com/rudra.rvu"><i class="fab fa-instagram"></i></a></li>
                        <li><a href="#"><i class="fab fa-linkedin"></i></a></li>
                        <li><a href="#"><i class="fab fa-github"></i></a></li>
                        <li><a href="#"><i class="fab fa-youtube"></i></a></li>
                        <li><a href="#"><i class="fab fa-twitter"></i></a></li>
                    </ul>
                </div>
                <div class="col-md-3">
                    <h5>Contact</h5>
                    <ul class="list-unstyled">
                        <li><a href="mailto:info@example.com"><i class="far fa-envelope"></i> Email</a></li>
                        <li>
                            <a href="mailto:contact.rudrarvu@gmail.com?cc=sujayvk.btech23@rvu.edu.in,adityas.btech23@rvu.edu.in,vaishnavs.btech23@rvu.edu.in">
                                <i class="far fa-envelope"></i> Contact Executive Board
                            </a>
                        </li>
                    </ul>
                </div>
            </div>
        </div>
    </footer>
    <!-- Footer End -->

    <script src="https://code.jquery.com/jquery-3.5.1.min.js"></script>
    <script src="https://cdn.jsdelivr.net/npm/popper.js@1.16.0/dist/umd/popper.min.js"></script>
    <script src="https://stackpath.bootstrapcdn.com/bootstrap/4.5.2/js/bootstrap.min.js"></script>
    <script type="text/javascript" async src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/MathJax.js?config=TeX-MML-AM_CHTML"></script>
 <script>
        document.addEventListener('DOMContentLoaded', function() {
            const options = { year: 'numeric', month: 'long', day: 'numeric' };
            const currentDate = new Date().toLocaleDateString('en-US', options);
            document.getElementById('current-date')?.textContent = currentDate;
        });

        function blackhole(element) {
            var h = $(element).height(),
                w = $(element).width(),
                cw = w,
                ch = h,
                maxorbit = 255,
                centery = ch / 2,
                centerx = cw / 2;

            var startTime = new Date().getTime();
            var currentTime = 0;
            var stars = [];

            var canvas = $('<canvas/>').attr({ width: cw, height: ch }).appendTo(element),
                context = canvas.get(0).getContext("2d");

            function rotate(cx, cy, x, y, angle) {
                var radians = angle,
                    cos = Math.cos(radians),
                    sin = Math.sin(radians),
                    nx = (cos * (x - cx)) + (sin * (y - cy)) + cx,
                    ny = (cos * (y - cy)) - (sin * (x - cx)) + cy;
                return [nx, ny];
            }

            var star = function () {
                var rands = [];
                rands.push(Math.random() * (maxorbit / 2) + 1);
                rands.push(Math.random() * (maxorbit / 2) + maxorbit);

                this.orbital = (rands.reduce((p, c) => p + c, 0) / rands.length);
                this.x = centerx;
                this.y = centery + this.orbital;
                this.speed = (Math.floor(Math.random() * 2.5) + 1.5) * Math.PI / 180;
                this.rotation = 0;
                this.startRotation = (Math.floor(Math.random() * 360) + 1) * Math.PI / 180;
                this.id = stars.length;
                stars.push(this);
            }

            star.prototype.draw = function () {
                this.rotation = this.startRotation + (currentTime * this.speed);

                context.save();
                context.beginPath();
                var oldPos = rotate(centerx, centery, this.prevX, this.prevY, -this.prevR);
                context.moveTo(oldPos[0], oldPos[1]);
                context.translate(centerx, centery);
                context.rotate(this.rotation);
                context.translate(-centerx, -centery);
                context.lineTo(this.x, this.y);
                context.stroke();
                context.restore();

                this.prevR = this.rotation;
                this.prevX = this.x;
                this.prevY = this.y;
            }

            $(window).on('resize', function () {
                h = $(element).height();
                w = $(element).width();
                cw = w;
                ch = h;
                centerx = cw / 2;
                centery = ch / 2;
                canvas.attr({ width: cw, height: ch });
                stars = [];
                for (var i = 0; i < 2000; i++) new star();
            });

            function loop() {
                var now = new Date().getTime();
                currentTime = (now - startTime) / 50;
                context.fillStyle = 'rgba(25,25,25,0.2)';
                context.fillRect(0, 0, cw, ch);
                for (var i = 0; i < stars.length; i++) stars[i].draw();
                requestAnimationFrame(loop);
            }

            function init() {
                for (var i = 0; i < 2000; i++) new star();
                loop();
            }

            init();
        }
</script>
</body>
</html>
