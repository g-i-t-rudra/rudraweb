<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>RUDRA Weekly Knowledge Drop</title>
    <link href="https://fonts.googleapis.com/css2?family=Poppins:wght@300;400;600&display=swap" rel="stylesheet">
    <style>
        body, html {
            height: 100%;
            margin: 0;
            overflow-x: hidden;
            font-family: 'Poppins', sans-serif;
            color: #e8f1f2;
        }
        
        #blackhole {
            position: fixed;
            top: 0;
            left: 0;
            width: 100%;
            height: 100%;
            z-index: -1;
        }
        
        canvas {
            width: 100%;
            height: 100%;
        }
        
        .content-wrapper {
            position: relative;
            z-index: 1;
        }
        
        
        .container {
            max-width: 1000px;
            margin: 24px auto;
            background: rgba(20, 20, 30, 0.85);
            padding: 24px;
            border-radius: 12px;
            box-shadow: 0 8px 16px rgba(0, 0, 0, 0.8);
        }
        
        .knowledge-drop-header img {
            width: 100%;
            max-height: 450px;
            object-fit: cover;
            border-bottom: 5px solid #68d4f2;
            border-radius: 12px 12px 0 0;
        }
        
        .knowledge-drop-content {
            margin-left: 4vw;
            margin-right: 4vw;
            margin-top: 16px;
        }
        
        .glowing-title {
            text-align: center;
            font-size: 2.5rem;
            font-weight: 600;
            color: #68d4f2;
            margin-top: 20px;
            animation: glowEffect 3s infinite alternate;
        }
        
        @keyframes glowEffect {
            0% {
                text-shadow: 0 0 5px rgba(104, 212, 242, 0.3), 0 0 10px rgba(104, 212, 242, 0.2);
                opacity: 0.7;
            }
            50% {
                text-shadow: 0 0 20px rgba(104, 212, 242, 0.8), 0 0 30px rgba(104, 212, 242, 0.5);
                opacity: 1;
            }
            100% {
                text-shadow: 0 0 5px rgba(104, 212, 242, 0.3), 0 0 10px rgba(104, 212, 242, 0.2);
                opacity: 0.7;
            }
        }
        
        .date-display {
            display: flex;
            align-items: center;
            justify-content: center;
            margin-top: 8px;
            color: #b9e3e5;
        }
        
        .date-display svg {
            margin-right: 8px;
        }
        
        .subtitle {
            text-align: center;
            margin-top: 16px;
            color: #e8f1f2;
            font-style: italic;
            font-size: 1.25rem;
        }
        
        h2 {
            color: #68d4f2;
            font-weight: bold;
            font-size: 1.5rem;
            margin-top: 24px;
            text-transform: uppercase;
        }
        
        p, li {
            line-height: 1.75;
            font-size: 1.125rem;
            margin-bottom: 20px;
        }
        
        ul {
            list-style-type: disc;
            padding-left: 24px;
        }
        
        strong {
            font-weight: 600;
        }
        
        .math-formula {
            background: rgba(104, 212, 242, 0.1);
            padding: 16px;
            border-radius: 8px;
            border-left: 4px solid #68d4f2;
            margin: 20px 0;
            overflow-x: auto;
        }
        
        .reference-list {
            border-top: 1px solid rgba(104, 212, 242, 0.3);
            margin-top: 40px;
            padding-top: 20px;
        }
        
        .reference-list li {
            font-size: 0.95rem;
            margin-bottom: 10px;
        }
    </style>
</head>
<body>
{% include 'navbar.html' %}
    <div id="blackhole"></div>
    
    <div class="content-wrapper">
        <div class="container">
            <div class="knowledge-drop-header">
                <img src="https://images.unsplash.com/photo-1620712943543-bcc4688e7485?ixlib=rb-1.2.1&auto=format&fit=crop&w=1350&q=80" alt="AI Self-Learning Concept">
            </div>

            <div class="knowledge-drop-content">
                <h1 class="glowing-title">RUDRA Weekly Knowledge Drop</h1>
                
                <div class="date-display">
                    <svg xmlns="http://www.w3.org/2000/svg" width="16" height="16" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round">
                        <circle cx="12" cy="12" r="10"></circle>
                        <polyline points="12 6 12 12 16 14"></polyline>
                    </svg>
                    <h5 id="current-date"></h5>
                </div>
                
                <h2 class="subtitle">Autonomous Self-Play in Large Language Models (LLMs) – The Future of AI Learning</h2>
                
                <h2>Introduction</h2>
                <p>
                    Self-play is a technique from reinforcement learning (RL) where an agent trains by playing against former (or
                    parallel) copies of itself. By adjusting the opponent's skill level in tandem with the agent's own improvements,
                    self-play enables stable and incremental policy optimization. Historically used in competitive
                    game-playing systems, self-play has recently been applied to develop advanced reasoning in
                    Large Language Models (LLMs). For instance, DeepSeek-R1 applies large-scale RL and self-play to refine
                    complex reasoning capabilities.
                </p>
                
                <h2>How Self-Play Works in LLMs</h2>
                <ol>
            <li>
                <b>Initialization:</b> The model (policy) \( \pi_{\theta_{\text{old}}} \) begins with either a base checkpoint or a lightly fine-tuned version.
            </li>
            <li>
                <b>Data Generation (Opponent Sampling):</b> Given a query \( q \), sample multiple outputs \( \{ o_i \}_{i=1}^{G} \) from the old policy \( \pi_{\theta_{\text{old}}} \).  
                Here, \( G \) represents the <i>group size</i>. For each query \( q \), we sample \( G \) distinct outputs \( \{ o_i \}_{i=1}^{G} \)  
                from the old policy \( \pi_{\theta_{\text{old}}} \). These outputs collectively provide a baseline against which each output’s relative advantage \( A_i \) is measured.
            </li>
            <li>
                <b>Scoring (Rewards):</b> For each output \( o_i \), compute a reward \( r_i \) based on correctness, format adherence, etc.
            </li>
            <li>
                <b>Policy Update:</b> Optimizes a new policy \( \pi_{\theta} \) to maximize the RL objective under self-play.  
                A particularly efficient framework, which is implemented in DeepSeek-R1, is <i>Group Relative Policy Optimization (GRPO)</i>.
            </li>
            <li>
                <b>Iterate and Refresh Opponent:</b> Replace \( \pi_{\theta_{\text{old}}} \) with the newly updated policy \( \pi_{\theta} \) and repeat.
            </li>
        </ol>
                
                <h2>The Mathematical Framework</h2>
                <p>
                    To <b>save the training costs of RL</b>, GRPO doesn’t make use of the large critic model and instead leverages a <i>group</i> of outputs to estimate the baseline.  
            Specifically, for each query \( q \), we draw a group of \( G \) outputs from the <i>old</i> policy \( \pi_{\theta_{\text{old}}} \)  
            and then optimize \( \pi_{\theta} \) by maximizing the following objective function:
                </p>
                
                <div class="math-formula">
                    <p>
  \( J_{\text{GRPO}}(\theta) = \mathbb{E}_{q \sim P(Q), \{o_i\}_{i=1}^{G} \sim \pi_{\theta_{\text{old}}}(O | q)}
  \left[ \frac{1}{G} \sum_{i=1}^{G} \left( \min \left( \frac{\pi_{\theta}(o_i | q)}{\pi_{\theta_{\text{old}}}(o_i | q)} A_i, \text{clip} \left( \frac{\pi_{\theta}(o_i | q)}{\pi_{\theta_{\text{old}}}(o_i | q)}, 1 - \epsilon, 1 + \epsilon \right) A_i \right) \right) 
  - \beta D_{\text{KL}} \left( \pi_{\theta} \parallel \pi_{\text{ref}} \right) \right] \)
</p>
                </div>
                
                <p>
                    In this formula:
                </p>
                <ul>
            <li>
                <b>\( q \)</b>: A query drawn from some distribution \( P(Q) \).
            </li>
            <li>
                <b>\( o_i \)</b>: An output sampled from the old policy \( \pi_{\theta_{\text{old}}} \).
            </li>
            <li>
                <b>\( \pi_{\theta}(o_i | q) \) and \( \pi_{\theta_{\text{old}}}(o_i | q) \)</b>: The probabilities of generating \( o_i \) under the 
                <i>new</i> and <i>old</i> policies, respectively.
                These are like the chances of scoring a goal using your new technique versus your old technique.
                You want the new method to be better but not totally different.
                The ratio of these two can be thought of as comparing your current performance to your past performance. 
                A ratio greater than 1 means you’re doing better, much like scoring more often than before.
            </li>
            <li>
                <b>\( A_i \)</b>: The <i>advantage</i> of output \( o_i \), computed as:
                <div>
                    \[
                    A_i = \frac{r_i - \text{mean}(\{r_1, \dots, r_G\})}{\text{std}(\{r_1, \dots, r_G\})}
                    \]
                </div>
                indicating how much better \( o_i \) is than the group-average outcome.
                It is essentially similar to measuring how much a particular goal stands out compared to your usual play.
            </li>
            <li>
                <b>\(\text{clip}(\cdot, 1 - \epsilon, 1 + \epsilon)\)</b>: Constrains the likelihood ratio to the interval \([1 - \epsilon, 1 + \epsilon]\),
                preventing destabilizing updates (it prevents the update from being too drastic).
            </li>
            <li>
                <b>\( \beta \)</b>: A hyperparameter controlling the strength of the KL-penalty term.
            </li>
            <li>
                <b>\( D_{\text{KL}}(\pi_{\theta} \parallel \pi_{\text{ref}}) \)</b>: The Kullback–Leibler divergence between \( \pi_{\theta} \) and a reference policy \( \pi_{\text{ref}} \), penalizing large deviations:
                <div>
                    \[
                    D_{\text{KL}}(\pi_{\theta} \parallel \pi_{\text{ref}}) = \sum_{i} \pi_{\text{ref}}(o_i | q) \log \left( \frac{\pi_{\text{ref}}(o_i | q)}{\pi_{\theta}(o_i | q)} \right) - 1.
                    \]
                </div>
                It essentially measures how much a model probability distribution \( Q \) is different from a true probability distribution \( P \),
                penalizing strategies that overly deviate from a trusted baseline.
            </li>
        </ul>
                
                <h2>Why Self-Play Matters</h2>
                <p>
                    Self-play offers several key advantages in LLM development:
                </p>
                <ul>
                    <li>
                        <strong>Adaptive Difficulty:</strong> An iteratively evolving opponent ensures the model is neither ”overwhelmed” nor ”under-challenged”. This is important because it ensures a gradual development of the model, which would otherwise get skewed on both the ends.
                    </li>
                    <li>
                        <strong>"Emergent" Reasoning:</strong> Models discover sophisticated behaviors (like self-correction) through iterative improvement without explicit programming.
                    </li>
                    <li>
                        <strong>Data Efficiency:</strong> Self-generated data dramatically reduces reliance on large supervised corpora, lowering training costs.
                    </li>
                    <li>
                        <strong>Broader Applications:</strong> The technique can be applied to robotics, self-driving vehicles, and medical AI systems.
                    </li>
                </ul>
                
                <h2>Challenges and Considerations</h2>
                <p>
                    Despite its promise, self-play in LLMs faces several challenges:
                </p>
                <ul>
                    <li>
                        <strong>Reward Hacking:</strong> Imperfect reward designs can lead to unintended solutions or behaviors that maximize rewards without achieving the desired goals.
                    </li>
                    <li>
                        <strong>Readability vs. Performance:</strong> Pure RL outputs may optimize for performance at the expense of human readability, requiring additional fine-tuning.
                    </li>
                    <li>
                        <strong>KL Regularization Balancing:</strong> Overly strong KL constraints can slow learning, while weak constraints may lead to instability or divergence.
                    </li>
                </ul>
                
                <h2>The Future of Self-Play in AI</h2>
                <p>
                    Self-play offers a structured approach for LLMs to enhance their reasoning skills autonomously. By integrating group-based advantage estimates and conservative policy updates, we can implement concrete self-play frameworks that drive AI advancement. This approach is already showing results in models like DeepSeek-R1, suggesting that self-play could become a foundational technique in the next generation of language models.
                </p>
                <p>
                    As AI systems continue to evolve, self-play may enable more sophisticated forms of reasoning, problem-solving, and adaptation without requiring ever-larger datasets or computational resources. This could democratize AI advancement and lead to more capable, efficient, and robust language models in the future.
                </p>
                
                <div class="reference-list">
                    <h2>References</h2>
                    <ol>
                        <li>DeepSeek-AI (2025). DeepSeek-R1:<i> Incentivizing Reasoning Capability in LLMs via Reinforcement Learning.</i> arXiv:2501.12948.</li>
                        <li>Silver, D. et al. (2017). Mastering the game of Go without human knowledge. Nature, 550(7676), 354–359.</li>
                        <li>Samuel, A. L. (1959).<i> Some studies in machine learning using the game of checkers. IBM Journal of Research and Development</i>, 3(3), 210–229.</li>
                        <li>Andrew Cohen (2020).<i> Training Intelligent Adversaries Using Self-Play with ML-Agents.</i>
<a href="https://unity.com/blog/engine-platform/training-intelligent-adversaries-using-self-play-with-ml-agents">https://unity.com/blog/engine-platform/training-intelligent-adversaries-using-self-play-with-ml-agents.</a></li>
                        <li>Arturas, Vaitaitis.<i> Self-Play: Towards Autonomous Learning in LLMs.</i><br>
                        <a href="https://medium.com/@varturas/self-play-towards-autonomous-learning-in-llms-78969458e3a3">https://medium.com/@varturas/self-play-towards-autonomous-learning-in-llms-78969458e3a3.</a>
                        </li>
                        <li>HuggingFace,<i> Deep RL Course - Self-Play.</i><br>
                        <a href="https://huggingface.co/learn/deep-rl-course/en/unit7/self-play">https://huggingface.co/learn/deep-rl-course/en/unit7/self-play.</a></li>
                    </ol>
                </div>
            </div>
        </div>
    </div>

    <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.6.0/jquery.min.js"></script>
    <script type="text/javascript" async
  src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/MathJax.js?config=TeX-MML-AM_CHTML">
</script>

    <script>
        // Set the current date
        document.addEventListener('DOMContentLoaded', function() {
            const options = { year: 'numeric', month: 'long', day: 'numeric' };
            const currentDate = new Date().toLocaleDateString('en-US', options);
            document.getElementById('current-date').textContent = currentDate;
            
            // Initialize black hole
            blackhole('#blackhole');
        });
        
        // Black hole animation
        function blackhole(element) {
            var h = $(element).height(),
                w = $(element).width(),
                cw = w,
                ch = h,
                maxorbit = 255, // distance from center
                centery = ch/2,
                centerx = cw/2;

            var startTime = new Date().getTime();
            var currentTime = 0;

            var stars = [],
                collapse = false, // if hovered
                expanse = false; // if clicked

            var canvas = $('<canvas/>').attr({width: cw, height: ch}).appendTo(element),
                context = canvas.get(0).getContext("2d");

            context.globalCompositeOperation = "lighter";

            function setDPI(canvas, dpi) {
                // Set up CSS size if it's not set up already
                if (!canvas.get(0).style.width)
                    canvas.get(0).style.width = canvas.get(0).width + 'px';
                if (!canvas.get(0).style.height)
                    canvas.get(0).style.height = canvas.get(0).height + 'px';

                var scaleFactor = dpi / 96;
                canvas.get(0).width = Math.ceil(canvas.get(0).width * scaleFactor);
                canvas.get(0).height = Math.ceil(canvas.get(0).height * scaleFactor);
                var ctx = canvas.get(0).getContext('2d');
                ctx.scale(scaleFactor, scaleFactor);
            }

            function rotate(cx, cy, x, y, angle) {
                var radians = angle,
                    cos = Math.cos(radians),
                    sin = Math.sin(radians),
                    nx = (cos * (x - cx)) + (sin * (y - cy)) + cx,
                    ny = (cos * (y - cy)) - (sin * (x - cx)) + cy;
                return [nx, ny];
            }

            setDPI(canvas, 192);

            var star = function(){
                // Get a weighted random number, so that the majority of stars will form in the center of the orbit
                var rands = [];
                rands.push(Math.random() * (maxorbit/2) + 1);
                rands.push(Math.random() * (maxorbit/2) + maxorbit);

                this.orbital = (rands.reduce(function(p, c) {
                    return p + c;
                }, 0) / rands.length);
                // Done getting that random number, it's stored in this.orbital

                this.x = centerx; // All of these stars are at the center x position at all times
                this.y = centery + this.orbital; // Set Y position starting at the center y + the position in the orbit

                this.yOrigin = centery + this.orbital;  // this is used to track the particles origin

                this.speed = (Math.floor(Math.random() * 2.5) + 1.5)*Math.PI/180; // The rate at which this star will orbit
                this.rotation = 0; // current Rotation
                this.startRotation = (Math.floor(Math.random() * 360) + 1)*Math.PI/180; // Starting rotation.  If not random, all stars will be generated in a single line.  

                this.id = stars.length;  // This will be used when expansion takes place.

                this.collapseBonus = this.orbital - (maxorbit * 0.7); // This "bonus" is used to randomly place some stars outside of the blackhole on hover
                if(this.collapseBonus < 0){ // if the collapse "bonus" is negative
                    this.collapseBonus = 0; // set it to 0, this way no stars will go inside the blackhole
                }

                stars.push(this);
                this.color = 'rgba(104, 212, 242,'+ (1 - ((this.orbital) / 255)) +')'; // Color the star with our theme color, but make it more transparent the further out it is generated

                this.hoverPos = centery + (maxorbit/2) + this.collapseBonus;  // Where the star will go on hover of the blackhole
                this.expansePos = centery + (this.id%100)*-10 + (Math.floor(Math.random() * 20) + 1); // Where the star will go when expansion takes place

                this.prevR = this.startRotation;
                this.prevX = this.x;
                this.prevY = this.y;
            }
            
            star.prototype.draw = function(){
                // the stars are not actually moving on the X axis in my code.  I'm simply rotating the canvas context for each star individually so that they all get rotated with the use of less complex math in each frame.

                if(!expanse){
                    this.rotation = this.startRotation + (currentTime * this.speed);
                    if(!collapse){ // not hovered
                        if(this.y > this.yOrigin){
                            this.y-= 2.5;
                        }
                        if(this.y < this.yOrigin-4){
                            this.y+= (this.yOrigin - this.y) / 10;
                        }
                    } else { // on hover
                        this.trail = 1;
                        if(this.y > this.hoverPos){
                            this.y-= (this.hoverPos - this.y) / -5;
                        }
                        if(this.y < this.hoverPos-4){
                            this.y+= 2.5;
                        }
                    }
                } else {
                    this.rotation = this.startRotation + (currentTime * (this.speed / 2));
                    if(this.y > this.expansePos){
                        this.y-= Math.floor(this.expansePos - this.y) / -140;
                    }
                }

                context.save();
                context.fillStyle = this.color;
                context.strokeStyle = this.color;
                context.beginPath();
                var oldPos = rotate(centerx,centery,this.prevX,this.prevY,-this.prevR);
                context.moveTo(oldPos[0],oldPos[1]);
                context.translate(centerx, centery);
                context.rotate(this.rotation);
                context.translate(-centerx, -centery);
                context.lineTo(this.x,this.y);
                context.stroke();
                context.restore();

                this.prevR = this.rotation;
                this.prevX = this.x;
                this.prevY = this.y;
            }

            // Handle window resize
            $(window).on('resize', function(){
                h = $(element).height();
                w = $(element).width();
                cw = w;
                ch = h;
                centerx = cw/2;
                centery = ch/2;
                
                canvas.attr({width: cw, height: ch});
                setDPI(canvas, 192);
                stars = [];
                
                context.fillStyle = 'rgba(25,25,25,1)';
                context.fillRect(0, 0, cw, ch);
                for(var i = 0; i < 2000; i++){
                    new star();
                }
            });

            window.requestFrame = (function(){
                return  window.requestAnimationFrame       ||
                    window.webkitRequestAnimationFrame ||
                    window.mozRequestAnimationFrame    ||
                    function( callback ){
                    window.setTimeout(callback, 1000 / 60);
                };
            })();

            function loop(){
                var now = new Date().getTime();
                currentTime = (now - startTime) / 50;

                context.fillStyle = 'rgba(25,25,25,0.2)'; // somewhat clear the context, this way there will be trails behind the stars 
                context.fillRect(0, 0, cw, ch);

                for(var i = 0; i < stars.length; i++){  // For each star
                    if(stars[i] != stars){
                        stars[i].draw(); // Draw it
                    }
                }

                requestFrame(loop);
            }

            function init(){
                context.fillStyle = 'rgba(25,25,25,1)';  // Initial clear of the canvas, to avoid an issue where it all gets too dark
                context.fillRect(0, 0, cw, ch);
                for(var i = 0; i < 2000; i++){  // create 2000 stars
                    new star();
                }
                loop();
            }
            
            init();
            
            // Add hover effect to the content area
            $('.container').on('mouseover', function(){
                collapse = true;
            }).on('mouseout', function(){
                collapse = false;
            });
        }
    </script>
    {% include 'footer.html' %}
</body>
</html>
